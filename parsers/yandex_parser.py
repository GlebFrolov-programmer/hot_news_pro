import json
import os
import random
import time
from abc import ABC
from datetime import datetime
from urllib.parse import urlencode, urlparse, parse_qs
from typing import List, Dict, Optional

import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

from parsers.base_parser import BaseParser
from news.news_item import NewsItem
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError


class YandexParser(BaseParser, ABC):
    def __init__(self, requests_to_parse: list[str], parameters: dict, metadata: dict, save_to: dict):
        super().__init__()
        self.class_name = 'Yandex'
        self.requests_to_parse = requests_to_parse
        self.metadata = metadata
        self.parameters = parameters
        self.driver = None
        self.setup_driver()

        try:
            self.raw_data = [i for i in list(set(self.parse()))]
        except RetryError as e:
            print(f"Parsing failed after retries: {e}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
            self.raw_data = []
        finally:
            self.close_driver()

        self.save_to = save_to

        if save_to['TO_EXCEL']:
            self.to_excel()
        if save_to['TO_JSON']:
            self.to_json()
        self.print_statistics()

    @property
    def class_name(self) -> str:
        return self._class_name

    @class_name.setter
    def class_name(self, value: str):
        self._class_name = value

    @property
    def raw_data(self) -> list:
        return self._raw_data

    @raw_data.setter
    def raw_data(self, value: list):
        self._raw_data = value

    @property
    def requests_to_parse(self) -> list[str]:
        return self._requests_to_parse

    @requests_to_parse.setter
    def requests_to_parse(self, value: list[str]):
        self._requests_to_parse = value

    @property
    def metadata(self) -> dict:
        return self._metadata

    @metadata.setter
    def metadata(self, value: dict):
        self._metadata = value

    @property
    def parameters(self) -> dict:
        return self._parameters

    @parameters.setter
    def parameters(self, value: dict):
        self._parameters = value

    def setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞ Selenium"""
        try:
            chrome_options = Options()

            # Stealth-–æ–ø—Ü–∏–∏
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")

            # User-Agent
            chrome_options.add_argument(
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)

            # –°–∫—Ä—ã–≤–∞–µ–º WebDriver
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
            raise WebDriverException(f"Driver setup failed: {e}")

    def close_driver(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            self.driver.quit()
            print("‚úÖ –î—Ä–∞–π–≤–µ—Ä –∑–∞–∫—Ä—ã—Ç")

    def random_sleep(self, min_time: float, max_time: float):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
        sleep_time = random.uniform(min_time, max_time)
        time.sleep(sleep_time)
        return sleep_time

    def get_timings(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç–∞–π–º–∏–Ω–≥–æ–≤"""
        default_timings = {
            'page_load': 15,
            'element_wait': 15,
            'typing_delay_min': 0.01,
            'typing_delay_max': 0.03,
            'between_queries_min': 2,
            'between_queries_max': 3,
            'after_search_min': 3,
            'after_search_max': 3,
            'between_pages_min': 1,
            'between_pages_max': 2,
        }

        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∞–π–º–∏–Ω–≥–æ–≤ –∏–∑ parameters
        if self.parameters.get('timings'):
            default_timings.update(self.parameters['timings'])

        return default_timings

    def perform_search(self, query: str, timings: Dict) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ø–Ω–¥–µ–∫—Å–µ"""
        try:
            print(f"üîç –ü–æ–∏—Å–∫: '{query}'")

            # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            params = {
                'text': query,
                # 'lr': 213,  # –ú–æ—Å–∫–≤–∞ –∏ –æ–±–ª–∞—Å—Ç—å
                'p': 0,  # —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                'within': 1  # 2 –Ω–µ–¥–µ–ª–∏
            }

            base_url = "https://yandex.ru/search/"
            search_url = f"{base_url}?{urlencode(params)}"

            print(f"üåê –û—Ç–∫—Ä—ã–≤–∞–µ–º –Ø–Ω–¥–µ–∫—Å: {search_url}")
            self.driver.get(search_url)

            print("‚è≥ –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            WebDriverWait(self.driver, timings['page_load']).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".serp-item"))
            )
            print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞
            pause = self.random_sleep(
                timings['after_search_min'],
                timings['after_search_max']
            )
            print(f"‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞: {pause:.1f} —Å–µ–∫")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
            return False

    def is_advertisement(self, item) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ–∫–ª–∞–º–Ω—ã–º"""
        try:
            item.find_element(By.CSS_SELECTOR, ".label_theme_ad")
            return True
        except NoSuchElementException:
            return False

    def extract_title_and_url(self, item) -> tuple[Optional[str], Optional[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Å—Å—ã–ª–∫–∏ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            title_element = item.find_element(By.CSS_SELECTOR, ".OrganicTitle-Link, .serp-item__title")
            title = title_element.text.strip()
            url = title_element.get_attribute("href")
            return title, url

        except NoSuchElementException:
            try:
                title_element = item.find_element(By.CSS_SELECTOR, "h2 a")
                title = title_element.text.strip()
                url = title_element.get_attribute("href")
                return title, url
            except:
                return None, None

    def extract_date_info(self, item) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        try:
            date_element = item.find_element(By.CSS_SELECTOR, ".OrganicTextContentSpan, .datetime")
            return date_element.text.strip()
        except NoSuchElementException:
            return "–î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

    def parse_page(self) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        results = []

        try:
            # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            search_items = self.driver.find_elements(By.CSS_SELECTOR, ".serp-item")
            print(f"üìã –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(search_items)}")

            for i, item in enumerate(search_items, 1):
                try:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    if self.is_advertisement(item):
                        print(f"  –≠–ª–µ–º–µ–Ω—Ç {i}: —Ä–µ–∫–ª–∞–º–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                        continue

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫—É
                    title, url = self.extract_title_and_url(item)

                    if title and url:
                        date_info = self.extract_date_info(item)

                        results.append({
                            'title': title,
                            'url': url,
                            'date': date_info,
                        })

                        print(f"  {i}. {title[:60]}...")

                except Exception as e:
                    print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —ç–ª–µ–º–µ–Ω—Ç–∞ {i}: {e}")
                    continue

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")

        return results

    def navigate_to_page(self, query: str, page_number: int, timings: Dict) -> bool:
        """–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ p –≤ URL"""
        try:
            if page_number == 1:
                return True  # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

            print(f"üìÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}...")

            params = {
                'text': query,
                # 'lr': 213,  # –ú–æ—Å–∫–≤–∞ –∏ –æ–±–ª–∞—Å—Ç—å
                'p': page_number - 1,  # —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω—É–º–µ—Ä—É—é—Ç—Å—è —Å 0
                'within': 1  # 2 –Ω–µ–¥–µ–ª–∏
            }

            base_url = "https://yandex.ru/search/"
            search_url = f"{base_url}?{urlencode(params)}"

            print(f"üåê –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ URL: {search_url}")
            self.driver.get(search_url)

            print("‚è≥ –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            WebDriverWait(self.driver, timings['page_load']).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".serp-item"))
            )

            self.random_sleep(timings['between_pages_min'], timings['between_pages_max'])
            print(f"‚úì –£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}: {e}")
            return False

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=8, max=15),
        retry=retry_if_exception_type((WebDriverException, TimeoutException))
    )
    def parse(self) -> list[NewsItem]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""

        print(f'\nüîé YANDEX SCRAPING {self.metadata}')

        if not self.driver:
            raise WebDriverException("–î—Ä–∞–π–≤–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        news_items = []
        timings = self.get_timings()
        total_queries = len(self.requests_to_parse)

        for i, request in enumerate(self.requests_to_parse, 1):
            query = request['query'] if isinstance(request, dict) else request

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if isinstance(request, dict) and 'search_limit' in request:
                max_results = request['search_limit']
            else:
                max_results = self.parameters.get('SEARCH_LIMIT_YANDEX', 15)

            print(f'    [{i}/{total_queries}] QUERY: {query}')
            print(f'    –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {max_results}')

            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                if self.perform_search(query, timings):
                    all_results = []
                    page = 0

                    # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                    while len(all_results) < max_results:
                        page += 1
                        print(f"      üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}")

                        # –ü–∞—Ä—Å–∏–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        page_results = self.parse_page()
                        all_results.extend(page_results)

                        print(f"      üìä –ù–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(page_results)}")
                        print(f"      üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_results)}")

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                        if len(all_results) >= max_results:
                            all_results = all_results[:max_results]
                            print(f"      ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                            break

                        # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                        if not self.navigate_to_page(query, page + 1, timings):
                            print(f"      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                            break

                    # –°–æ–∑–¥–∞–µ–º NewsItem –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    for j, result in enumerate(all_results, 1):
                        news_items.append(
                            NewsItem(
                                source=self.class_name,
                                metadata=self.metadata,
                                url=result['url'],
                                title=result['title'],
                                approved=self.check_approved_source(result['url'])
                            )
                        )
                        print(f"        {j}. {result['title'][:70]}...")

                    print(f"      ‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_results)}")
                else:
                    print(f"      ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫")

            except Exception as e:
                print(f"Error processing query '{query}': {e}")
                raise e

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < total_queries:
                pause = self.random_sleep(
                    timings['between_queries_min'],
                    timings['between_queries_max']
                )
                print(f"      ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {pause:.1f} —Å–µ–∫...")

        return news_items