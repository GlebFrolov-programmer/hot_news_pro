import json
import os
import random
import time
from abc import ABC
import asyncio
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from parsers.base_parser import BaseParser
from news.news_item import NewsItem
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError
from selenium.common.exceptions import WebDriverException
from urllib.parse import urlparse, parse_qs


class GoogleParser(BaseParser, ABC):
    def __init__(self, requests_to_parse: list[str], parameters: dict, metadata: dict, save_to: dict):
        super().__init__()
        self.class_name = 'Google'
        self.requests_to_parse = requests_to_parse
        self.metadata = metadata
        self.parameters = parameters
        self.driver = None
        self.setup_driver()

        try:
            self.raw_data = [i for i in list(set(self.parse()))]
        except RetryError as e:
            print(f"Parsing failed after retries: {e}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
            self.raw_data = []
        finally:
            self.close_driver()

        self.save_to = save_to

        if save_to['TO_EXCEL']:
            self.to_excel()
        if save_to['TO_JSON']:
            self.to_json()
        self.print_statistics()

    @property
    def class_name(self) -> str:
        return self._class_name

    @class_name.setter
    def class_name(self, value: str):
        self._class_name = value

    @property
    def raw_data(self) -> list:
        return self._raw_data

    @raw_data.setter
    def raw_data(self, value: list):
        self._raw_data = value

    @property
    def requests_to_parse(self) -> list[str]:
        return self._requests_to_parse

    @requests_to_parse.setter
    def requests_to_parse(self, value: list[str]):
        self._requests_to_parse = value

    @property
    def metadata(self) -> dict:
        return self._metadata

    @metadata.setter
    def metadata(self, value: dict):
        self._metadata = value

    @property
    def parameters(self) -> dict:
        return self._parameters

    @parameters.setter
    def parameters(self, value: dict):
        self._parameters = value

    def setup_driver(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞ Selenium"""
        try:
            chrome_options = Options()

            # Stealth-–æ–ø—Ü–∏–∏
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")

            # User-Agent
            chrome_options.add_argument(
                "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

            # Headless —Ä–µ–∂–∏–º (–º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)
            # chrome_options.add_argument("--headless=new")

            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)

            # –°–∫—Ä—ã–≤–∞–µ–º WebDriver
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
            raise WebDriverException(f"Driver setup failed: {e}")

    def close_driver(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞"""
        if self.driver:
            self.driver.quit()
            print("‚úÖ –î—Ä–∞–π–≤–µ—Ä –∑–∞–∫—Ä—ã—Ç")

    def random_sleep(self, min_time: float, max_time: float):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞"""
        sleep_time = random.uniform(min_time, max_time)
        time.sleep(sleep_time)
        return sleep_time

    def accept_cookies(self):
        """–ü—Ä–∏–Ω—è—Ç–∏–µ cookies"""
        try:
            cookie_selectors = [
                "//button[contains(., '–ü—Ä–∏–Ω—è—Ç—å –≤—Å–µ')]",
                "//button[contains(., 'Accept all')]",
                "//button[contains(., 'I agree')]",
            ]

            for selector in cookie_selectors:
                try:
                    cookie_button = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    cookie_button.click()
                    print("‚úì Cookie –ø—Ä–∏–Ω—è—Ç—ã")
                    return True
                except:
                    continue

            return False

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ cookie: {e}")
            return False

    def perform_search(self, query: str, timings: Dict) -> bool:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        try:
            print(f"üîç –ü–æ–∏—Å–∫: '{query}'")
            print("üåê –û—Ç–∫—Ä—ã–≤–∞–µ–º Google...")

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º Google
            self.driver.get("https://www.google.com")
            print("‚úì Google –∑–∞–≥—Ä—É–∂–µ–Ω")

            # self.random_sleep(2, 3)

            # –ü—Ä–∏–Ω–∏–º–∞–µ–º cookies
            # print("üç™ –ü—Ä–æ–≤–µ—Ä—è–µ–º cookies...")
            # if self.accept_cookies():
            #     print("‚úì Cookies –ø—Ä–∏–Ω—è—Ç—ã")
            # else:
            #     print("‚ö†Ô∏è Cookies –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

            # –ò—â–µ–º –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
            print("üîé –ò—â–µ–º –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É...")
            search_box = WebDriverWait(self.driver, timings['element_wait']).until(
                EC.presence_of_element_located((By.NAME, "q"))
            )
            print("‚úì –ü–æ–∏—Å–∫–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–∞–π–¥–µ–Ω–∞")

            # –û—á–∏—â–∞–µ–º –∏ –≤–≤–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å
            print("‚å®Ô∏è –í–≤–æ–¥–∏–º –∑–∞–ø—Ä–æ—Å...")
            search_box.clear()
            for char in query:
                search_box.send_keys(char)
                self.random_sleep(
                    timings['typing_delay_min'],
                    timings['typing_delay_max']
                )
            print("‚úì –ó–∞–ø–æ–ª–Ω–µ–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")

            # –ù–∞–∂–∏–º–∞–µ–º Enter
            print("‚èé –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å...")
            search_box.send_keys(Keys.RETURN)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print("‚è≥ –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            WebDriverWait(self.driver, timings['page_load']).until(
                EC.presence_of_element_located((By.ID, "search"))
            )
            print("‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞
            pause = self.random_sleep(
                timings['after_search_min'],
                timings['after_search_max']
            )
            print(f"‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–æ—Å–ª–µ –ø–æ–∏—Å–∫–∞: {pause:.1f} —Å–µ–∫")

            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
            return False

    def navigate_to_page(self, page_number: int, timings: Dict) -> bool:
        """–ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        try:
            if page_number == 1:
                return True  # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞

            print(f"üìÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}...")

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
            navigation_methods = [
                self._navigate_via_pagination_buttons,
                self._navigate_via_url_parameter
            ]

            for method in navigation_methods:
                if method(page_number, timings):
                    print(f"‚úì –£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ—à–ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}")
                    return True

            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}")
            return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number}: {e}")
            return False

    def _navigate_via_pagination_buttons(self, page_number: int, timings: Dict) -> bool:
        """–ù–∞–≤–∏–≥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        try:
            # –ò—â–µ–º –∫–Ω–æ–ø–∫–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            pagination_selectors = [
                f"//a[@aria-label='Page {page_number}']",
                f"//a[contains(text(), '{page_number}')]",
                "//td[@class='YyVfkd']/a",
                "//a[@id='pnnext']",
                "//a[contains(@href, 'start=')]"
            ]

            for selector in pagination_selectors:
                try:
                    page_buttons = WebDriverWait(self.driver, 3).until(
                        EC.presence_of_all_elements_located((By.XPATH, selector))
                    )

                    for button in page_buttons:
                        if str(page_number) in button.text or str(page_number) in button.get_attribute(
                                'aria-label') or '':
                            button.click()
                            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            WebDriverWait(self.driver, timings['page_load']).until(
                                EC.presence_of_element_located((By.ID, "search"))
                            )
                            self.random_sleep(1, 2)
                            return True

                except:
                    continue

            return False

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –∫–Ω–æ–ø–∫–∏: {e}")
            return False

    def _navigate_via_url_parameter(self, page_number: int, timings: Dict) -> bool:
        """–ù–∞–≤–∏–≥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä URL"""
        try:
            current_url = self.driver.current_url
            parsed_url = urlparse(current_url)
            query_params = parse_qs(parsed_url.query)

            # –í—ã—á–∏—Å–ª—è–µ–º start –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
            start = (page_number - 1) * 10

            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã URL
            query_params['start'] = [str(start)]

            # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–π URL
            new_query = '&'.join([f"{k}={v[0]}" for k, v in query_params.items()])
            new_url = f"{parsed_url.scheme}://{parsed_url.netloc}{parsed_url.path}?{new_query}"

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –ø–æ –Ω–æ–≤–æ–º—É URL
            self.driver.get(new_url)

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            WebDriverWait(self.driver, timings['page_load']).until(
                EC.presence_of_element_located((By.ID, "search"))
            )

            self.random_sleep(1, 2)
            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ URL: {e}")
            return False

    def extract_links(self) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        links = []
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            selectors = ["div.g a", "div.tF2Cxc a", "div.MjjYud a", "h3 a"]

            for selector in selectors:
                results = soup.select(selector)
                if results:
                    for result in results:
                        href = result.get('href', '')
                        if (href and href.startswith('http') and
                                'google.com' not in href and
                                not href.startswith('/')):
                            title = result.get_text(strip=True) or "No title"

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å URL
                            if not any(link['url'] == href for link in links):
                                links.append({
                                    'url': href,
                                    'title': title[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
                                })
                    break

            return links

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {e}")
            return []

    def get_timings(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç–∞–π–º–∏–Ω–≥–æ–≤"""
        default_timings = {
            'page_load': 5,
            'element_wait': 3,
            'typing_delay_min': 0.01,
            'typing_delay_max': 0.03,
            'between_queries_min': 1,
            'between_queries_max': 2,
            'after_search_min': 1,
            'after_search_max': 2,
            'between_pages_min': 1,
            'between_pages_max': 3,
        }

        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç–∞–π–º–∏–Ω–≥–æ–≤ –∏–∑ parameters
        if self.parameters.get('timings'):
            default_timings.update(self.parameters['timings'])

        return default_timings

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=8, max=15),
        retry=retry_if_exception_type((WebDriverException, TimeoutException))
    )
    def parse(self) -> list[NewsItem]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""

        print(f'\nGOOGLE SCRAPING {self.metadata}')

        if not self.driver:
            raise WebDriverException("–î—Ä–∞–π–≤–µ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        news_items = []
        timings = self.get_timings()
        total_queries = len(self.requests_to_parse)

        for i, request in enumerate(self.requests_to_parse, 1):
            query = request['query'] if isinstance(request, dict) else request

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            if isinstance(request, dict) and 'search_limit' in request:
                max_results = request['search_limit']
            else:
                max_results = self.parameters.get('SEARCH_LIMIT_GOOGLE', 10)

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
            pages_to_scrape = (max_results + 9) // 10  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö

            print(f'    [{i}/{total_queries}] QUERY: {query}, –°—Ç—Ä–∞–Ω–∏—Ü: {pages_to_scrape}, –õ–∏–º–∏—Ç: {max_results}')

            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
                if self.perform_search(query, timings):
                    all_links = []

                    # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
                    for page in range(1, pages_to_scrape + 1):
                        print(f"      üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{pages_to_scrape}")

                        # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –Ω—É–∂–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (–¥–ª—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–µ—Ä–µ—Ö–æ–¥ –Ω–µ –Ω—É–∂–µ–Ω)
                        if page > 1:
                            if not self.navigate_to_page(page, timings):
                                print(f"      ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                                break

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        page_links = self.extract_links()
                        print(f"      üìã –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(page_links)}")

                        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                        for link in page_links:
                            if not any(l['url'] == link['url'] for l in all_links):
                                all_links.append(link)

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –ª–∏–º–∏—Ç–∞
                        if len(all_links) >= max_results:
                            all_links = all_links[:max_results]
                            print(f"      ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                            break

                        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                        if page < pages_to_scrape:
                            pause = self.random_sleep(
                                timings['between_pages_min'],
                                timings['between_pages_max']
                            )
                            print(f"      ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏: {pause:.1f} —Å–µ–∫...")

                    # –°–æ–∑–¥–∞–µ–º NewsItem –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
                    for j, link in enumerate(all_links, 1):
                        news_items.append(
                            NewsItem(
                                source=self.class_name,
                                metadata=self.metadata,
                                url=link['url'],
                                title=link['title'],
                                approved=self.check_approved_source(link['url'])
                            )
                        )
                        print(f"        {j}. {link['title']}")

                    print(f"      ‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_links)}")
                else:
                    print(f"      ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫")

            except Exception as e:
                print(f"Error processing query '{query}': {e}")
                raise e

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            if i < total_queries:
                pause = self.random_sleep(
                    timings['between_queries_min'],
                    timings['between_queries_max']
                )
                print(f"      ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏: {pause:.1f} —Å–µ–∫...")

        return news_items




#  –ò–ù–°–¢–†–£–ú–ï–ù–¢ SCRAPER.DO

# import json
# import os
# import random
# import time
# from abc import ABC
# import asyncio
# import pandas as pd
# import requests
# import urllib.parse
# from typing import List, Dict, Optional
# from parsers.base_parser import BaseParser
# from news.news_item import NewsItem
# from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, RetryError
# from urllib.parse import urlparse, parse_qs, quote
#
#
# class GoogleParser(BaseParser, ABC):
#     def __init__(self, requests_to_parse: list[str], parameters: dict, metadata: dict, save_to: dict):
#         super().__init__()
#         self.class_name = 'Google'
#         self.requests_to_parse = requests_to_parse
#         self.metadata = metadata
#         self.parameters = parameters
#         self.scrape_do_token = parameters.get('SCRAPE_DO_TOKEN', '')
#
#         if not self.scrape_do_token:
#             raise ValueError("SCRAPE_DO_TOKEN is required for GoogleParser")
#
#         try:
#             self.raw_data = [i for i in list(set(self.parse()))]
#         except RetryError as e:
#             print(f"Parsing failed after retries: {e}, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
#             self.raw_data = []
#
#         self.save_to = save_to
#
#         if save_to['TO_EXCEL']:
#             self.to_excel()
#         if save_to['TO_JSON']:
#             self.to_json()
#         self.print_statistics()
#
#     @property
#     def class_name(self) -> str:
#         return self._class_name
#
#     @class_name.setter
#     def class_name(self, value: str):
#         self._class_name = value
#
#     @property
#     def raw_data(self) -> list:
#         return self._raw_data
#
#     @raw_data.setter
#     def raw_data(self, value: list):
#         self._raw_data = value
#
#     @property
#     def requests_to_parse(self) -> list[str]:
#         return self._requests_to_parse
#
#     @requests_to_parse.setter
#     def requests_to_parse(self, value: list[str]):
#         self._requests_to_parse = value
#
#     @property
#     def metadata(self) -> dict:
#         return self._metadata
#
#     @metadata.setter
#     def metadata(self, value: dict):
#         self._metadata = value
#
#     @property
#     def parameters(self) -> dict:
#         return self._parameters
#
#     @parameters.setter
#     def parameters(self, value: dict):
#         self._parameters = value
#
#     def random_sleep(self, min_time: float, max_time: float):
#         """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
#         sleep_time = random.uniform(min_time, max_time)
#         time.sleep(sleep_time)
#         return sleep_time
#
#     def make_scrape_do_request(self, url: str) -> Optional[str]:
#         """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ scrape.do API"""
#         try:
#             # –ö–æ–¥–∏—Ä—É–µ–º URL –¥–ª—è API
#             encoded_url = quote(url)
#             api_url = f"http://api.scrape.do/?url={encoded_url}&token={self.scrape_do_token}"
#
#             # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
#             # if self.parameters.get('SCRAPE_DO_PREMIUM'):
#                 # api_url += "&premium=true"
#             # if self.parameters.get('SCRAPE_DO_RENDER'):
#             #     api_url += "&render=true"
#
#             headers = {
#                 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
#             }
#
#             response = requests.get(api_url, headers=headers, timeout=30)
#             response.raise_for_status()
#
#             return response.text
#
#         except requests.exceptions.RequestException as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ scrape.do API –¥–ª—è {url}: {e}")
#             return None
#
#     def build_google_search_url(self, query: str, page: int = 1) -> str:
#         """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ URL –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ Google"""
#         # –ö–æ–¥–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
#         encoded_query = quote(query)
#
#         # –í—ã—á–∏—Å–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä start –¥–ª—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
#         start = (page - 1) * 10
#
#         # –°–æ–±–∏—Ä–∞–µ–º URL
#         base_url = "https://www.google.com/search"
#         url = f"{base_url}?q={encoded_query}&num=10"
#
#         if start > 0:
#             url += f"&start={start}"
#
#         return url
#
#     def extract_links_from_html(self, html: str) -> List[Dict]:
#         """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google"""
#         from bs4 import BeautifulSoup
#
#         links = []
#         try:
#             soup = BeautifulSoup(html, 'html.parser')
#
#             # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Google
#             selectors = ["div.g a", "div.tF2Cxc a", "div.MjjYud a", "h3 a"]
#
#             for selector in selectors:
#                 results = soup.select(selector)
#                 if results:
#                     for result in results:
#                         href = result.get('href', '')
#
#                         # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º Google-—Ä–µ–¥–∏—Ä–µ–∫—Ç—ã
#                         if href.startswith('/url?q='):
#                             href = href.split('/url?q=')[1].split('&')[0]
#                             href = urllib.parse.unquote(href)
#
#                         if (href and href.startswith('http') and
#                                 'google.com' not in href and
#                                 not href.startswith('/')):
#                             title = result.get_text(strip=True) or "No title"
#
#                             # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å URL
#                             if not any(link['url'] == href for link in links):
#                                 links.append({
#                                     'url': href,
#                                     'title': title[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
#                                 })
#                     break
#
#             return links
#
#         except Exception as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –∏–∑ HTML: {e}")
#             return []
#
#     def get_timings(self) -> Dict:
#         """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç–∞–π–º–∏–Ω–≥–æ–≤"""
#         default_timings = {
#             'between_queries_min': 2,
#             'between_queries_max': 5,
#             'between_pages_min': 1,
#             'between_pages_max': 3,
#         }
#
#         if self.parameters.get('timings'):
#             default_timings.update(self.parameters['timings'])
#
#         return default_timings
#
#     @retry(
#         stop=stop_after_attempt(3),
#         wait=wait_exponential(multiplier=1, min=8, max=15),
#         retry=retry_if_exception_type((requests.exceptions.RequestException,))
#     )
#     def parse(self) -> list[NewsItem]:
#         """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º scrape.do API"""
#
#         print(f'\nGOOGLE SCRAPING (scrape.do) {self.metadata}')
#         print(f'Using token: {self.scrape_do_token[:10]}...')
#
#         news_items = []
#         timings = self.get_timings()
#         total_queries = len(self.requests_to_parse)
#
#         for i, request in enumerate(self.requests_to_parse, 1):
#             query = request['query'] if isinstance(request, dict) else request
#
#             # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
#             if isinstance(request, dict) and 'search_limit' in request:
#                 max_results = request['search_limit']
#             else:
#                 max_results = self.parameters.get('SEARCH_LIMIT_GOOGLE', 10)
#
#             # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
#             pages_to_scrape = (max_results + 9) // 10  # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –≤–≤–µ—Ä—Ö
#
#             print(f'    [{i}/{total_queries}] QUERY: {query}, –°—Ç—Ä–∞–Ω–∏—Ü: {pages_to_scrape}, –õ–∏–º–∏—Ç: {max_results}')
#
#             try:
#                 all_links = []
#
#                 # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
#                 for page in range(1, pages_to_scrape + 1):
#                     print(f"      üìñ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{pages_to_scrape}")
#
#                     # –°—Ç—Ä–æ–∏–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞
#                     search_url = self.build_google_search_url(query, page)
#                     print(f"      üîó URL: {search_url}")
#
#                     # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ scrape.do
#                     html_content = self.make_scrape_do_request(search_url)
#
#                     if not html_content:
#                         print(f"      ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}")
#                         continue
#
#                     # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ HTML
#                     page_links = self.extract_links_from_html(html_content)
#                     print(f"      üìã –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(page_links)}")
#
#                     # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
#                     for link in page_links:
#                         if not any(l['url'] == link['url'] for l in all_links):
#                             all_links.append(link)
#
#                     # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –ª–∏–º–∏—Ç–∞
#                     if len(all_links) >= max_results:
#                         all_links = all_links[:max_results]
#                         print(f"      ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ {max_results} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
#                         break
#
#                     # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
#                     if page < pages_to_scrape:
#                         pause = self.random_sleep(
#                             timings['between_pages_min'],
#                             timings['between_pages_max']
#                         )
#                         print(f"      ‚è≥ –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏: {pause:.1f} —Å–µ–∫...")
#
#                 # –°–æ–∑–¥–∞–µ–º NewsItem –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–∏
#                 for j, link in enumerate(all_links, 1):
#                     news_items.append(
#                         NewsItem(
#                             source=self.class_name,
#                             metadata=self.metadata,
#                             url=link['url'],
#                             title=link['title'],
#                             approved=self.check_approved_source(link['url'])
#                         )
#                     )
#                     print(f"        {j}. {link['title']}")
#
#                 print(f"      ‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(all_links)}")
#
#             except Exception as e:
#                 print(f"Error processing query '{query}': {e}")
#                 # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
#                 continue
#
#         return news_items
